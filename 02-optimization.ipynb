{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization methods for single and multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optimization methods allows you to find local/global maxima/minima maybe under some restrictions. These methods can be quite elaborated and might need to be optimized. For this reason, it is advisable to use libraries tailored to these kind of tasks. \n",
    "\n",
    "For a simple example of optimization, consider a bungee jumper that jumps upward at a specified velocity (example from Chapra Chapter 7). If the jumper is subjected to drag, its altitude $z$ can be computed as \n",
    "\n",
    "\\begin{equation}\n",
    "z = z_0 + \\frac{m}{c}\\left( v_0 + \\frac{mg}{c} \\right) (1 - e^{-(c/m)t}) - \\frac{mg}{c}t,\n",
    "\\end{equation}\n",
    "where $c$ is the drag coefficient. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we use $g = 9.81$ m/s$^2$, $z_0 = 100$ m, $v_0 = 55$ m/s, $m = 80$ kg, and $c = 15$ kg/s, one obtains something like \n",
    "<img src=\"figs/example-z.png\" width=600>\n",
    "\n",
    "\n",
    "You might answer:\n",
    "* How to find this maximum programatically? \n",
    "* How to find how it depends on $c$? \n",
    "these are the questions that an optimization method can answer.\n",
    "\n",
    "In `scipy`, the general `optimization` module gives many tools to optimize, for intance, a multi-variate function under constrains. See the general docs: https://docs.scipy.org/doc/scipy/reference/optimize.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization details\n",
    "1. Root finding\n",
    "2. One-dimensional or multi-dimensional\n",
    "3. Local and global extrema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Root finding?\n",
    "The problem of finding the maximum/minimum of a function is related with the problem of root finding, but it is not equivalent, since we are looking for roots on the derivatives, not the function itself, as the following picture shows:\n",
    "<img src=\"figs/opt-01.png\" style=\"width:600px\">\n",
    "\n",
    "As you can see, you need both the first and second derivative to make sure you are finding a maximum/minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## One dimensional and multi-dimensional optimization\n",
    "You can also have the problem of finding an extremum that depends on several variables (like the maximum height $z$ as a function of both $c$ and $m$), as the next figure shows\n",
    "<img src=\"figs/opt-02.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Local and global extrema\n",
    "And, finally, you need to take into account the possible existence of local and global maximum/minimum, as illustrated in the following\n",
    "<img src=\"figs/opt-03.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One-dimensional optimization\n",
    "1. Traditional methods\n",
    "2. Example with scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Traditional methods (one-dimensional optimization)\n",
    "* Golden ratio search : Based on the golden ratio number, similar to binary search.\n",
    "* Parabolic interpolation : Uses interpolation to minimize the evaluation of the function.\n",
    "* Newton-Raphson : Applies the same Newton-Raphson method but for the derivatives.\n",
    "* Brent method: Same as the previous (apply brents method to the derivatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## One-dimensional/Uni-variate optimization with `scipy`\n",
    "`Scipy` offers the module `optmize` to perform optimization tasks, and inside it you can use several methods (read the docs for each one):\n",
    "* `golden` : Uses the golden ratio method inside some interval. Returns the minimim of a given function.\n",
    "* `brent` : Improved method that converges faster than `golden`\n",
    "* `fminbound` : Allows to restrict the solution to a given interval.\n",
    "* `minimize_scalar` : General method to minimize a scalar function, where you can select the method to use as argument (`golden`, `brent`, etc). The solution is a bunch object with much more information. **You should use this function to solve uni-variate problems.** You cand find the docs at https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar  \n",
    "\n",
    "In the following you can find some examples for `minimize_scalar`, extracted from the actual manual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:24:51.803075Z",
     "start_time": "2019-08-23T19:24:51.784836Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# function to minimize\n",
    "def f(x):\n",
    "    #return np.where(x <= 0, -x, (x - 2) * x * (x + 2)**2)\n",
    "    return (x - 2) * x * (x + 2)**2\n",
    "\n",
    "# import the function\n",
    "from scipy.optimize import minimize_scalar\n",
    "res = minimize_scalar(f)\n",
    "x1 = res.x\n",
    "print(\"The solution is : \", x1)\n",
    "\n",
    "# And this is a bounded method to find the solution inside some given interval\n",
    "res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
    "x2 = res.x\n",
    "print(\"The bounded solution is : \", x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:25:49.520681Z",
     "start_time": "2019-08-23T19:25:49.113004Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Let's plot the function\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(); sns.set_context('poster')\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-3.5, 2.5, 200)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, f(x))\n",
    "ax.plot(x1, f(x1), 'rs')\n",
    "ax.plot(x2, f(x2), 'go')\n",
    "ax.add_patch(plt.Rectangle((-3, -5), 2, 10, alpha=0.2, fill=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise \n",
    "Use the previous methods to compute the minimum of the next function,\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{x^2}{10} - 2\\sin x\n",
    "\\end{equation}\n",
    "Plot the function and verify that all methods are finding the right minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-13T23:55:23.999318Z",
     "start_time": "2018-09-13T23:55:23.431008Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "Compute the minimum for the function\n",
    "\\begin{equation}\n",
    "f(x) = x^2(1+3\\cos x\\  e^{-(x-0.1)^2/10})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:26:28.005252Z",
     "start_time": "2019-08-23T19:26:27.557208Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# function to minimize\n",
    "def f(x):\n",
    "    #return np.where(x <= 0, -x, (x - 2) * x * (x + 2)**2)\n",
    "    return x*x*(1 + 3*np.cos(x)*np.exp(-(x-0.1)**2/10))\n",
    "\n",
    "# import the function\n",
    "from scipy.optimize import minimize_scalar\n",
    "res = minimize_scalar(f)\n",
    "x1 = res.x\n",
    "print(\"The solution is : \", x1)\n",
    "\n",
    "%matplotlib inline\n",
    "# Let's plot the function\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(); sns.set_context('poster')\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-3.5, 2.5, 200)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, f(x))\n",
    "ax.plot(x1, f(x1), 'rs')\n",
    "#ax.plot(x2, f(x2), 'go')\n",
    "#ax.add_patch(plt.Rectangle((-3, -5), 2, 10, alpha=0.2, fill=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise (from Miguel Uribe)\n",
    "\n",
    "<img src=\"figs/F1.png\" alt=\"Annotation Examples\" style=\"width:400px\">\n",
    "\n",
    "Suppose you plan to go from point $A$ to point $B$. $A$ is separated from $B$ by a total of 20880m, however, to go from $A$ to $B$ you have to travel a distance $L_A$ using a car and a distance $L_B$ using a boat. $x$ marks the horizontal distance travelled before you change your transportation vehicle.\n",
    "\n",
    "While travelling on land using a car you can move with a speed of $v_A=20\\,\\rm{km/h}$, whereas while travelling on water with the boat you move with a speed of $v_B=10\\,\\rm{km/h}$. On the other hand each kilometer on land costs $\\gamma_A=25000 \\,\\rm{COP/km}$ and each kilometer on water costs $\\gamma_B=15000 \\,\\rm{COP/km}$.\n",
    "\n",
    "The total transportation cost to go from $A$ to $B$ would be:\n",
    "$$\\tau_T=\\gamma_AL_A+\\gamma_BL_B.$$\n",
    "However, your time is also valuable and there is a cost for each second you take in the travel. The time cost is given by the parameter $\\beta$, which is given in $\\rm{COP/h}$. The time cost of the trip is thus given by:\n",
    "$$\\tau_t=\\beta\\left(\\frac{L_A}{v_A}+\\frac{L_B}{v_B}\\right)$$\n",
    "\n",
    "Therefore, the total cost of the travel is the addition of the transportation and time costs:\n",
    "$$\\tau=L_A\\left(\\gamma_A+\\frac{\\beta}{v_A}\\right)+L_B\\left(\\gamma_B+\\frac{\\beta}{v_B}\\right)$$\n",
    "\n",
    "Finally, the total cost can be written in terms of the distance $x$ using Pythagoras' theorem. $L_A=\\sqrt{(x^2+(3\\,\\rm{km})^2)}$ and $L_B=\\sqrt{(20\\,\\rm{km}-x)^2+(3\\,\\rm{km})^2)}$.\n",
    "\n",
    "Show in a Figure the value of $x$ for which the travel cost is minimized as a function of the time cost parameter $\\beta$. Scan $\\beta$ over the range between $0 \\,\\rm{COP/h}$ (you do not care about time) and $1000000\\,\\rm{COP/h}$ (you care a lot about time) using at least 1000 points.\n",
    "\n",
    "For the same range of $\\beta$, plot the optimal cost per trip as a function of the time cost and plot the travel time that minimizes the cost as a function of the time cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:28:45.686743Z",
     "start_time": "2019-08-23T19:28:45.317741Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "VA=20 # km/h\n",
    "VB=10 # km/h\n",
    "YA=25000 # COP/km\n",
    "YB=15000 # COP/km\n",
    "\n",
    "# function to minimize\n",
    "def f(x, beta):\n",
    "    LA=np.sqrt(x**2 + 9)\n",
    "    LB=np.sqrt((20-x)**2 + 9)\n",
    "    return LA*(YA + beta/VA) + LB*(YB + beta/VB)\n",
    "\n",
    "# import the function\n",
    "from scipy.optimize import minimize_scalar\n",
    "BETA=np.linspace(0, 1000000, 1000)\n",
    "xopt=np.zeros_like(BETA)\n",
    "for ii, b in enumerate(BETA):\n",
    "    res = minimize_scalar(f, args=(b,)) \n",
    "    xopt[ii] = res.x\n",
    "#print(BETA)\n",
    "#print(xopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:28:47.266584Z",
     "start_time": "2019-08-23T19:28:46.671920Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Let's plot the function\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(); sns.set_context('poster')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(BETA, xopt)\n",
    "ax.set_xlabel(r\"$\\beta$\")\n",
    "ax.set_ylabel(r\"$x*$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-variate methods (based on the work from Miguel Uribe)\n",
    "In this case your function depends on several variables so the solution method is much more elaborated. \n",
    "\n",
    "Please read https://www2.hawaii.edu/~jonghyun/classes/S18/CEE696/files/04_scipy_optimize.pdf for short introduction to the classification of multi-variate optimization problems.\n",
    "\n",
    "The general minimization method in the case of multivariate methods is the `minimize` method. As for the `minimize_scalar` method, in `minimize` the method to be used is passed as an argument. There are many available methods to minimize multivariate functions, once again, the method to be chosen depends on the available information about the function, the available computing capacity, and the required performance.\n",
    "\n",
    "The available methods are:\n",
    "- `Nelder-Mead`\n",
    "- `Powell`\n",
    "- `CG`\n",
    "- `BFGS`\n",
    "- `Newton-CG`\n",
    "- `L-BFGS-B`\n",
    "- `TNC`\n",
    "- `COBYLA`\n",
    "- `SLSQP`\n",
    "- `dogleg`\n",
    "- `trust-ncg`\n",
    "- `trust-exact`\n",
    "- `trust-krylov`\n",
    "   \n",
    "Some of these methods use the information of the gradients and second order derivatives of the function to find the minimum. However, we will focus in this section on the methods that do not use additional information apart from the function itself. These methods are useful in the cases where the optimizing function is not really known or it is simply too complicated to differentiate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BFGS - Broyden–Fletcher–Goldfarb–Shanno algorithm : Unconstrained\n",
    "\n",
    "The BFGS algorithm belongs to the family of quasi-Newton methods. These are methods which seek the minimum of a function be moving in the direction for which the gradient is the smallest. Normally, these methods need information regarding the gradient and the Hessian matrix (second order derivatives). However, the BFGS method is able to approximate the hessian using information regarding the gradient. If the gradient is not given either, the python implementation of the method also approximates the gradient based on different function evaluations.\n",
    "\n",
    "More information regarding the BFGS method can be found at: https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T02:30:51.794330Z",
     "start_time": "2020-08-26T02:30:51.250908Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Lets define the function to minimize\n",
    "def fun2D(x):\n",
    "    res=(x[0]-1)**4+5*(x[1]-1)**2-2*x[0]*x[1]\n",
    "    return res\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#matplotlib.rc('text', usetex=True)\n",
    "import seaborn as sns\n",
    "sns.set(); sns.set_context('poster')\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x_ = y_ = np.linspace(-1, 4, 100)  # Creating 1D arrays between -4 and 4 for x an y\n",
    "X, Y = np.meshgrid(x_, y_)  # With this comand we create a 100x100 2D mesh\n",
    "# We create the figure and also give it an alias to change its attributes\n",
    "c = ax.contour(X, Y, fun2D([X, Y]), 100, cmap='viridis') # The 100 is the number of countour lines\n",
    "#ax.plot(x_opt[0], x_opt[1], 'r*', markersize=15) # We plot the optimal values\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=18)  # we set the x label\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=18)  # we set the y label\n",
    "plt.colorbar(c)                # We add a colorbar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T02:30:58.718171Z",
     "start_time": "2020-08-26T02:30:58.697872Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "# Running the minimize method to find the minimum\n",
    "x0=[0,0]  # This is the initiall guess, which is mandatory\n",
    "results=minimize(fun2D, x0, method='BFGS')\n",
    "print(results)\n",
    "\n",
    "# We store the optimal as x_opt\n",
    "x_opt=results.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Checking the results using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-26T02:31:04.238317Z",
     "start_time": "2020-08-26T02:31:03.670935Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Countour plots in matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#matplotlib.rc('text', usetex=True)\n",
    "import seaborn as sns\n",
    "sns.set();\n",
    "sns.set_context('poster')\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x_ = y_ = np.linspace(-1, 4, 100)  # Creating 1D arrays between -4 and 4 for x an y\n",
    "X, Y = np.meshgrid(x_, y_)  # With this comand we create a 100x100 2D mesh\n",
    "# We create the figure and also give it an alias to change its attributes\n",
    "c = ax.contour(X, Y, fun2D([X, Y]), 100, cmap='viridis') # The 50 is the number of countour lines\n",
    "ax.plot(x_opt[0], x_opt[1], 'r*', markersize=15) # We plot the optimal values\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=18)  # we set the x label\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=18)  # we set the y label\n",
    "plt.colorbar(c)                # We add a colorbar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG - Conjugate gradient method\n",
    "The conjugate gradient method was originally proposed to iteratively solve linear equation systems but was later generalized to solve unconstrained non-linear multivariate optimization problems. It is based on an iterative approximation of the gradient of a function using the so-called conjugate vectors.\n",
    "\n",
    "More information regarding the CG method can be found at https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_conjugate_gradient_method_as_an_iterative_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:38:24.788982Z",
     "start_time": "2019-08-23T19:38:24.776104Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Running the minimize method to find the minimum\n",
    "x0=[0,0]  # This is the initiall guess, which is mandatory\n",
    "results=minimize(fun2D,x0, method='CG')\n",
    "print(results)\n",
    "\n",
    "# We store the optimal as x_opt\n",
    "x_opt=results.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:38:28.974159Z",
     "start_time": "2019-08-23T19:38:28.242819Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the results of the CG method\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "x_=y_=np.linspace(-1,4,100)  # Creating 1D arrays between -4 and 4 for x an y\n",
    "X, Y = np.meshgrid(x_, y_)  # With this comand we create a 100x100 2D mesh\n",
    "# We create the figure and also give it an alias to change its attributes\n",
    "c = ax.contour(X, Y, fun2D([X, Y]), 50, cmap=\"viridis\") # The 50 is the number of countour lines\n",
    "ax.plot(x_opt[0], x_opt[1], 'r*', markersize=15) # We plot the optimal values\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=18)  # we set the x label\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=18)  # we set the y label\n",
    "plt.colorbar(c, ax=ax)                # We add a colorbar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Nelder-Mead method\n",
    "The Nelder-Mead method or Downhill Simplex method is a more brute force method. It creates a simplex, a generalization of a triangle (in 2D) or a tetahedron (in 3D), for the given dimension. The corners of the simplex is evaluated and according to their values the simplex is either reflected, expanded, contracted or shrunk to find a better value. As a consequence, the simplex starts to move looking for the minimum value, once it finds the minimum it starts to shrink around it. The method normally converges when the size of the simplex is smaller than the expected tolerance.\n",
    "\n",
    "More information regarding the Nelder-Mead method can be found at https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:38:42.702074Z",
     "start_time": "2019-08-23T19:38:42.674789Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Running the minimize method to find the minimum\n",
    "x0=[0,0]  # This is the initiall guess, which is mandatory\n",
    "results=minimize(fun2D,x0, method='Nelder-Mead')\n",
    "print(results)\n",
    "\n",
    "# We store the optimal as x_opt\n",
    "x_opt=results.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:38:45.839997Z",
     "start_time": "2019-08-23T19:38:45.072886Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the results of the CG method\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "x_=y_=np.linspace(-1,4,100)  # Creating 1D arrays between -4 and 4 for x an y\n",
    "X, Y = np.meshgrid(x_, y_)  # With this comand we create a 100x100 2D mesh\n",
    "# We create the figure and also give it an alias to change its attributes\n",
    "c = ax.contour(X, Y, fun2D([X, Y]), 50, cmap='viridis') # The 50 is the number of countour lines\n",
    "ax.plot(x_opt[0], x_opt[1], 'r*', markersize=15) # We plot the optimal values\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=18)  # we set the x label\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=18)  # we set the y label\n",
    "plt.colorbar(c, ax=ax)                # We add a colorbar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It can be seen that the BFGS method is the one that requires the least amount of function evaluation, it is generally the most recomended method to start with multivariate minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What if the function is non-convex and there are several local minima.\n",
    "\n",
    "If there are local minima, the final result strongly depends on the initial guess. Regardless the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:40:44.357276Z",
     "start_time": "2019-08-23T19:40:44.342336Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define a 2D function with several local minima\n",
    "def func2D_NC(X):\n",
    "    x,y=X     # We define x,y as the contents of the X array\n",
    "    res=(4 * np.sin(np.pi * x) + 6 * np.sin(np.pi * y)) + (x - 1)**2 + (y - 1)**2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T19:40:46.248393Z",
     "start_time": "2019-08-23T19:40:44.957949Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Checking graphically the dependence of the BFGS method on the initial guess\n",
    "\n",
    "## We first plot the function\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x_=y_=np.linspace(-3,6,100)  # Creating 1D arrays between -4 and 4 for x an y\n",
    "X, Y = np.meshgrid(x_, y_)  # With this comand we create a 100x100 2D mesh\n",
    "# We create the figure and also give it an alias to change its attributes\n",
    "c = ax.contour(X, Y, func2D_NC([X, Y]), 30, cmap='viridis') # The 50 is the number of countour lines\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=18)  # we set the x label\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=18)  # we set the y label\n",
    "plt.colorbar(c, ax=ax)                # We add a colorbar\n",
    "\n",
    "# we perform several optimization routines with different initial guesses and plot the results on the same graph\n",
    "\n",
    "#Creating the list of initial points\n",
    "x0s=[[x,y] for x in np.arange(-3,6) for y in np.arange(-3,6)]\n",
    "\n",
    "for x0 in x0s:\n",
    "    results=minimize(func2D_NC,x0, method='BFGS')\n",
    "    xopt=results.x\n",
    "    # We plot the initial points\n",
    "    ax.plot(x0[0], x0[1], 'bo', markersize=6)\n",
    "    \n",
    "    # We plot the final results\n",
    "    ax.plot(xopt[0], xopt[1], 'r*', markersize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local and absolute extremum\n",
    "In fact, all methods tend to ger trapped in local minima. A possibility to overcome this problem is to perform the optimization process for a wide range of different possible initial guesses and then compare the results to find the global minima. This process, however, can lead to large computation times.\n",
    "\n",
    "An additional possibility is to evaluate the function in a fine mesh, find the minimum value and use this value as the initial guess of a more advanced optimization method. The `optimization.brute` routine allows for this search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T21:07:17.112201Z",
     "start_time": "2018-09-15T21:07:17.099539Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import brute\n",
    "# Performing the brute optimization over a network of points separated by 0.5\n",
    "x0appr=brute(func2D_NC, (slice(-3,6,0.5),slice(-3,6,0.5)), finish=None)\n",
    "# The slice objects passed as arguments are basically the x and y point distribution\n",
    "# The finish=None attribute stops the method at the first iteration, otherwise it will continue using finer and finer meshes to find the global minimum\n",
    "print(x0appr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The brute method finds the point $(x,y)=(1.5,1.5)$ as the one that minimizes the function in the initial grid. We can now use this point as the initial guess for the optimizing routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T21:08:06.373681Z",
     "start_time": "2018-09-15T21:08:05.851364Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## We first plot the function\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x_= y_ = np.linspace(-3, 6, 100)  # Creating 1D arrays between -4 and 4 for x an y\n",
    "X, Y = np.meshgrid(x_, y_)  # With this comand we create a 100x100 2D mesh\n",
    "# We create the figure and also give it an alias to change its attributes\n",
    "c = ax.contour(X, Y, func2D_NC([X, Y]), 20, cmap='viridis') # The 50 is the number of countour lines\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=18)  # we set the x label\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=18)  # we set the y label\n",
    "plt.colorbar(c, ax=ax)                # We add a colorbar\n",
    "\n",
    "# The BFGS method\n",
    "results=minimize(func2D_NC, x0appr, method='BFGS')\n",
    "xopt=results.x\n",
    "ax.plot(xopt[0], xopt[1], 'r*', markersize=15)\n",
    "\n",
    "# The CG method\n",
    "results=minimize(func2D_NC, x0appr, method='CG')\n",
    "xopt=results.x\n",
    "ax.plot(xopt[0], xopt[1], 'g*', markersize=10)\n",
    "\n",
    "\n",
    "# The Nelder-Mead method\n",
    "results=minimize(func2D_NC, x0appr, method='Nelder-Mead')\n",
    "xopt=results.x\n",
    "ax.plot(xopt[0], xopt[1], 'w*', markersize=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can see that using the `brute` method to determine the initial guess all methods find the actual global minimum of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate methods with constraints\n",
    "\n",
    "So far, the multivariate methods we have studied are unconstrained, which means there are no boundaries for the possible values of the variables.\n",
    "\n",
    "In practice, however, many optimization problems are constrained. Python deals differently with three different classes of constraints:\n",
    "- **bounds**: These are constraints of the type $x_i=[x_i,x_f]$. This is, the variable is restricted to a fixed range.\n",
    "- **equality constraints**: These are constraints of the type $h(x)=0$, notice that in this notation $x$ is a vector of variables. Therefore, any number of variables can appear in the constraint.\n",
    "- **inequality constraints**: These are constraints of the type $g(x)\\geq 0$, once again $x$ denotes a vector of variables. \n",
    "\n",
    "The following is an example extracted from the `minimize` function manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-15T21:19:51.087258Z",
     "start_time": "2018-09-15T21:19:51.075287Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def fun(x) : \n",
    "    return (x[0] - 1)**2 + (x[1] - 2.5)**2 # paraboloid centered at (1, 2.5)\n",
    "\n",
    "# Define constrains, as inequalities\n",
    "# the first one, for instance, implies that : x - 2y + 2 > 0\n",
    "cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
    "        {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
    "        {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
    "\n",
    "# define bounds: in tis case, all variables are positive\n",
    "bnds = ((0, None), (0, None))\n",
    "\n",
    "# solve using the SLSQP method\n",
    "res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds, constraints=cons)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T16:09:22.527555Z",
     "start_time": "2018-09-14T16:09:22.179610Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot just to check\n",
    "## We first plot the function\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x_ = np.linspace(-2, 4, 100)  # Creating 1D arrays between -4 and 4 for x an y\n",
    "y_ = np.linspace(0, 5, 100)  # Creating 1D arrays between -4 and 4 for x an y\n",
    "X, Y = np.meshgrid(x_, y_)  # With this comand we create a 100x100 2D mesh\n",
    "# We create the figure and also give it an alias to change its attributes\n",
    "c = ax.contour(X, Y, fun([X, Y]), 20, cmap='viridis') # The 50 is the number of countour lines\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=18)  # we set the x label\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=18)  # we set the y label\n",
    "plt.colorbar(c, ax=ax)                # We add a colorbar\n",
    "\n",
    "ax.plot(res.x[0], res.x[1], 'r*', markersize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another example for multivariate problems with bounds\n",
    "When constraints are bound-type, which are the easiest to handle, the `L-BFGS-B` method is recommended. As already mentioned, this method estimates the gradient and the hessian of the optimized function to find the optimal value. In this modification, it also takes into account the given bounds for the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T16:10:25.474734Z",
     "start_time": "2018-09-14T16:10:25.471131Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define a new function to estimate\n",
    "def func2D(X):\n",
    "    x,y=X\n",
    "    return (x-1)**2+(y-1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now lets minimize this function without constraints and subjected to the constraints $2\\leq x\\leq 3$ and $0\\leq y \\leq 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T16:10:39.534864Z",
     "start_time": "2018-09-14T16:10:39.196700Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x0=[0,0]  # This is the initiall guess, which is mandatory\n",
    "\n",
    "# optimizing without constraints\n",
    "res=minimize(func2D,x0, method='BFGS')\n",
    "print(res.x)\n",
    "\n",
    "# optimizing with the constrains, we define the boundaries as tuples\n",
    "bnd1,bnd2=(2,3),(0,2)\n",
    "res_cons=minimize(func2D,x0, method='L-BFGS-B', bounds=[bnd1,bnd2])\n",
    "print(res_cons.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To get a better understanding of the results, we can plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T16:12:00.550974Z",
     "start_time": "2018-09-14T16:11:59.655831Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## We first plot the function\n",
    "fig, axes1 = plt.subplots(figsize=(10, 8))   # We create the axes to plot\n",
    "x_=y_=np.linspace(-1,3,100)  # Creating 1D arrays between -4 and 4 for x an y\n",
    "X, Y = np.meshgrid(x_, y_)  # With this comand we create a 100x100 2D mesh\n",
    "# We create the figure and also give it an alias to change its attributes\n",
    "c = axes1.contour(X, Y, func2D([X, Y]), 50, cmap='viridis') # The 50 is the number of countour lines\n",
    "axes1.set_xlabel(r\"$x$\", fontsize=18)  # we set the x label\n",
    "axes1.set_ylabel(r\"$y$\", fontsize=18)  # we set the y label\n",
    "plt.colorbar(c, ax=axes1)                # We add a colorbar\n",
    "\n",
    "# Plotting the result for the unconstrained method\n",
    "axes1.plot(res.x[0],res.x[1], 'r*', markersize=15)\n",
    "\n",
    "# Plotting the result for the unconstrained method\n",
    "axes1.plot(res_cons.x[0],res_cons.x[1], 'b*', markersize=15)\n",
    "\n",
    "# Lets highlight the constrained region\n",
    " # we define a rectangle: Rectangle((origin), width,height)\n",
    "bound=plt.Rectangle((bnd1[0],bnd2[0]),bnd1[1]-bnd1[0],bnd2[1]-bnd2[0], facecolor=\"grey\")\n",
    "axes1.add_patch(bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We see both minimization methods are succesful in finding the minimum under the given constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another eample for multivariate problems with equality or inequality constraints\n",
    "\n",
    "Dealing with equality or inequaliy constraints is a much more complicated problem and, once again, there is not a magical recipe. The most succesful analytical method is known as Lagrange multipliers, which turn the problem into an unconstrained one by adding more variables. The sequential least squares programming `SLSQP` method incorporates the Lagrange multipliers method and extends it to take into account equality and inequality constraints.\n",
    "\n",
    "To introduce constraints into the `minimize` method, a list of dictionaries must be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's suppose we want to find the minimal value of the function above, `func2D`. This time we are restricted to the values of $x$ and $y$ which satisfy the following condition:\n",
    "$$g(X)=y-1.75-(x-0.75)^4\\geq 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T16:13:17.127745Z",
     "start_time": "2018-09-14T16:13:17.110791Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#We start by defining the function g(X) used in the constraint\n",
    "def gcons(X):\n",
    "    x,y=X\n",
    "    return y-1.75-(x-0.75)**4\n",
    "\n",
    "# We create a dictionary to define the constraint, with at least two keys: fun and type\n",
    "const={'type':'ineq', 'fun':gcons}\n",
    "\n",
    "# We now call the optimization with and without constrains\n",
    "# optimizing without constraints\n",
    "res=minimize(func2D,x0, method='BFGS')\n",
    "print(res.x)\n",
    "\n",
    "# optimizing with the constraint g(x)>=0\n",
    "res_cons=minimize(func2D,x0, method='SLSQP', constraints=const)\n",
    "print(res_cons.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once again we can get a better understanding of the solutions in a figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-14T16:22:48.784109Z",
     "start_time": "2018-09-14T16:22:48.258147Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## We first plot the function\n",
    "fig, axes1 = plt.subplots(figsize=(10, 8))   # We create the axes to plot\n",
    "x_=y_=np.linspace(-1,3,100)  # Creating 1D arrays between -4 and 4 for x an y\n",
    "X, Y = np.meshgrid(x_, y_)  # With this comand we create a 100x100 2D mesh\n",
    "axes1.set_ylim(-1,3)         # Forces the limits of y to be between (-1,3)\n",
    "# We create the figure and also give it an alias to change its attributes\n",
    "c = axes1.contour(X, Y, func2D([X, Y]), 50, cmap='viridis') # The 50 is the number of countour lines\n",
    "axes1.set_xlabel(r\"$x$\", fontsize=18)  # we set the x label\n",
    "axes1.set_ylabel(r\"$y$\", fontsize=18)  # we set the y label\n",
    "plt.colorbar(c, ax=axes1)                # We add a colorbar\n",
    "\n",
    "# Plotting the result for the unconstrained method\n",
    "axes1.plot(res.x[0],res.x[1], 'r*', markersize=15)\n",
    "\n",
    "# Plotting the result for the constrained method\n",
    "axes1.plot(res_cons.x[0],res_cons.x[1], 'b*', markersize=15)\n",
    "\n",
    "# Lets highlight the constrained region\n",
    "axes1.plot(x_, 1.75 + (x_-0.75)**4, 'k-', markersize=15)   # Adds the black line\n",
    "axes1.fill_between(x_,1.75+(x_-0.75)**4, 3, color='grey')  # Adds the grey shadowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Once again, the minimization procedure works well."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
